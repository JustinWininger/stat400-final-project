---
title: "STAT 400 Final Project"
author: "Justin Wininger, Michael Ulis, Zachary Fuller, Collin Lu"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(randomForest)
library(rattle)
library(rpart)
library(lubridate)
library(kableExtra)
library(readxl)
library(car)
library(caret)
library(pROC)
library(lme4)
```

## Front Matter

```{r}
qb_stats <- read.csv('NFL QB Stats.csv')
allQB <- read.csv('allQBs.csv')
qb_summary <- qb_stats %>%
group_by(Player) %>%
summarise(
years_played = n(),
total_yards = sum(Pass.Yds)
)
```

# Chapter 1: GLM

## 1.1 Research Question

Is it possible to use various career stats from a National Football League (NFL) quarterback (QB) to predict if he made the Pro Football Hall of Fame (HOF)? What stats are the best for prediction?

## 1.2 Data

```{r}
qb <- qb_stats %>% 
  group_by(Player) %>%
  summarise(
    seasons_played = n(),
    pass_yards = sum(Pass.Yds),
    Cmp = sum(Cmp),
    Att = sum(Att),
    `Cmp%` = round(((sum(Cmp) * 100) / sum(Att)), 1),
    TD = sum(TD),
    INT = sum(INT),
    first_downs = sum(X1st),
    twenty_plus = sum(`X20.`),
    forty_plus = sum(`X40.`),
    Lng = max(Lng),
    Sck = sum(Sck),
    Scky = sum(SckY)
  )


allQB <- allQB %>%
  mutate(
    HOF = ifelse(grepl("\\+$", Player), "Yes", "No"),
    Player = gsub("\\+$", "", Player)
  )

allQB <- allQB[,-(2:3)]
allQB <- allQB[,-(4)]

qb_hof <- inner_join(allQB, qb, by=join_by(Player))
allqb_hof <- inner_join(allQB, qb, by=join_by(Player))

allqb_hof2<- inner_join(allQB, qb, by=join_by(Player))

qb_hof <- qb_hof %>% filter(To <  2016, From >= 1970, seasons_played >= 10, pass_yards > 25000, TD > 150)

qb_hof <- qb_hof[,c(1,4,2,3,5:17)]

qb_hof <- qb_hof %>% mutate(HOF_ind = ifelse(HOF == 'Yes', 1, 0))

get_major_decade <- function(start, end) {
  decades <- seq(floor(start / 10) * 10, floor(end / 10) * 10, by = 10)
  years_in_decade <- sapply(decades, function(d) min(end, d + 9) - max(start, d) + 1)
  return(decades[which.max(years_in_decade)])
}

qb_hof$Decade <- mapply(get_major_decade, qb_hof$From, qb_hof$To)

qb_hof$Decade <- paste0(qb_hof$Decade, "s")

qb_hof$TD <- as.integer(qb_hof$TD)
qb_hof$Decade <- as.factor(qb_hof$Decade)

allqb_hof <- allqb_hof %>% filter(To <  2016, From >= 1970)

allqb_hof <- allqb_hof[,c(1,4,2,3,5:17)]

allqb_hof <- allqb_hof %>% mutate(HOF_ind = ifelse(HOF == 'Yes', 1, 0))

get_major_decade <- function(start, end) {
  decades <- seq(floor(start / 10) * 10, floor(end / 10) * 10, by = 10)
  years_in_decade <- sapply(decades, function(d) min(end, d + 9) - max(start, d) + 1)
  return(decades[which.max(years_in_decade)])
}

allqb_hof$Decade <- mapply(get_major_decade, allqb_hof$From, allqb_hof$To)

allqb_hof$Decade <- paste0(allqb_hof$Decade, "s")

allqb_hof$TD <- as.integer(allqb_hof$TD)
allqb_hof$Decade <- as.factor(allqb_hof$Decade)

allqb_hof2 <- allqb_hof %>% filter(allqb_hof$Decade != '2010s')
```

### 1.2.1 Dataset Information

Our main dataset, `qb_hof`, comes from the join between two datasets we found online. The first dataset, `qb_stats` was from kaggle, <https://www.kaggle.com/datasets/supremeleaf/nfl-qb-stats-1970-2022>. The second dataset, `allQB` is from Pro Football Reference, <https://www.pro-football-reference.com/players/qbindex.htm>. The response variable we will use is `HOF` or maybe `HOF_ind`, depending on if the encoding is necessary. It is a categorical binomial variable, with levels 'Yes' and 'No' (or 1 and 0). It represents if the player in question made the HOF or not. Our other variables include:

| Variable | Explanation | Notes |
|:----------------|:---------------------------|----------------------------|
| `Player` | The player's name | Quantitative |
| `From` | The player's first season | Quantitative |
| `To` | The player's last season | Quantitative |
| `seasons_played` | How many seasons the player played | Quantitative |
| `pass_yards` | The player's career passing yards | Quantitative |
| `Cmp` | The player's career completions | Quantitative |
| `Att` | The player's career attempts | Quantitative |
| `Cmp%` | The player's career completion percentage | Quantitative, completion percentage = (Cmp \* 100) / Att |
| `TD` | The player's career passing touchdowns | Quantitative |
| `INT` | The player's career interceptions | Quantitative |
| `first_downs` | The player's career first downs | Quantitative |
| `twenty_plus` | The player's career throws for over a twenty yard gain | Quantitative |
| `forty_plus` | The player's career throws for over a forty yard gain | Quantitative |
| `Lng` | The player's career longest play | Quantitative |
| `Sck` | The number of times the player got sacked in their career | Quantitative |
| `Scky` | The total yardage lost from sacks in the player's career | Quantitative |
| `Decade` | The most common decade the player played in | Categorical, values: 1970s, 1980s, 1990s, 2000s |

### 1.2.2 Data Wrangling

The `qb_stats` dataset had the statline from every QB for every season between 1970 and 2022. To get it into the format we have now, we had to do several steps of data-wrangling. First, we grouped by player and summed all of the per-season stats. Others, like `Lng`, could not simply be summed, so we took the max. Because the dataset only contains the seasons between 1970 and 2022, it would confuse the model to include players who started before 1970 or ended after 2022. A player like Johnny Unitas, for example, is a HOF QB, but he is not included because he started his career in 1955. The dataset does not have his statistics from before 1970, so including him in the model would not be helpful. Additionally, there are several players like Eli Manning whose last seasons are before 2022 and are very likely to be in the HOF soon, but are not yet. These people could also throw off the model, so we decided to use our knowledge of football to remove such players. The last QB to make the HOF, Peyton Manning, retired in 2015, so that is the upper bound we decided on. The original dataset does not contain any career or HOF information, so that is where the second dataset, `allQB`, comes in. It contains a list of all players, with a '+' by their name if they are in the HOF, and the years they played. We created the column `HOF` with a 'Yes' if the player had the '+', and a 'No' if he did not. We performed an inner join to join these datasets, making sure to remove the '+' first. Unfortunately, there were only 13 QBs who fit our criteria who made the HOF, so in an effort to have similar numbers of HOF and non-HOF players, we filtered the dataset more. To only get experienced, well-known names, we decided to arbitrarily only include players with more than 10 seasons played, more than 25,000 passing yards, and more than 150 passing touchdowns. Next, we encoded `HOF` into `HOF_ind`. We then wrote a function to determine the primary decade a player played in. Finally, the dataset was wrangled and ready to use.

Below are several glimpses into the datatsets. They serve to help us identify underlying patterns in the data. 
```{r}
head(qb_hof)
```


```{r}
qb_eda <- qb_stats %>%
  group_by(Player) %>%
  summarise(
    yards_per_attempt = mean(Yds.Att),
    attempts = sum(Att),
    completions = sum(Cmp),
    completion_pct = mean(Cmp..),
    total_td = sum(TD),
    total_int = sum(INT),
    total_sacks = sum(Sck)
  )

head(qb_eda)
```

### 1.2.3 Possible Problems

Our dataset is very thorough, but it is not perfect. A large predictor of whether or not a player will make the HOF are his career awards. The NFL gives out several awards yearly, like the Most Valuable Player (MVP), Offensive Player of the Year (OPOY), and many others. These are very prestigious awards, and winning one goes a long way to making the HOF. Our dataset does not include these awards. Additionally, the dataset does not include championships like winning a Super Bowl, the NFL championship. The number of super bowl wins is often cited when discussing whether a player should make the HOF or not. 

## 1.3 EDA

Exploratory Data Analysis is a crucial step in any statistics project. It is necessary to observe the patterns and decide whether our chosen GLM fits the  conditions and makes sense. Below are several summary tables for some chosen variables that might be interesting. 

```{r}
table1 <- qb_summary %>%
  summarize(mean=mean(total_yards),
            min=min(total_yards),
            median=median(total_yards),
            max=max(total_yards),
            sd=sd(total_yards))

table1 %>%
  kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Summary Table for Total Yards Passed",
    col.names = c("Mean", "Minimum", "Median", "Maximum", "Standard Deviation"),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))
```

```{r}
table2 <- qb_summary %>%
  summarize(mean=mean(years_played),
            min=min(years_played),
            median=median(years_played),
            max=max(years_played),
            sd=sd(years_played))

table2 %>%
  kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Summary Table for Years Played",
    col.names = c("Mean", "Minimum", "Median", "Maximum", "Standard Deviation"),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))
```

```{r}
table3 <- qb_eda %>%
  summarize(mean=mean(yards_per_attempt),
            min=min(yards_per_attempt),
            median=median(yards_per_attempt),
            max=max(yards_per_attempt),
            sd=sd(yards_per_attempt))

table3 %>%
  kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Summary Table for Yards per Attempt",
    col.names = c("Mean", "Minimum", "Median", "Maximum", "Standard Deviation"),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))
```

```{r}
table4 <- qb_eda %>%
  summarize(mean=mean(attempts),
            min=min(attempts),
            median=median(attempts),
            max=max(attempts),
            sd=sd(attempts))

table4 %>%
  kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Summary Table for Pass Attempt",
    col.names = c("Mean", "Minimum", "Median", "Maximum", "Standard Deviation"),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))
```

```{r}
table5 <- qb_eda %>%
  summarize(mean=mean(completions),
            min=min(completions),
            median=median(completions),
            max=max(completions),
            sd=sd(completions))

table5 %>%
  kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Summary Table for Pass Completions",
    col.names = c("Mean", "Minimum", "Median", "Maximum", "Standard Deviation"),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))
```

```{r}
table6 <- qb_eda %>%
  summarize(mean=mean(completion_pct),
            min=min(completion_pct),
            median=median(completion_pct),
            max=max(completion_pct),
            sd=sd(completion_pct))

table6 %>%
  kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Summary Table for Pass Completion Percentage",
    col.names = c("Mean", "Minimum", "Median", "Maximum", "Standard Deviation"),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))
```

```{r}
table7 <- qb_eda %>%
  summarize(mean=mean(total_td),
            min=min(total_td),
            median=median(total_td),
            max=max(total_td),
            sd=sd(total_td))

table7 %>%
  kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Summary Table for Total Touchdowns",
    col.names = c("Mean", "Minimum", "Median", "Maximum", "Standard Deviation"),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))
```

```{r}
table8 <- qb_eda %>%
  summarize(mean=mean(total_int),
            min=min(total_int),
            median=median(total_int),
            max=max(total_int),
            sd=sd(total_int))

table8 %>%
  kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Summary Table for Total Interceptions",
    col.names = c("Mean", "Minimum", "Median", "Maximum", "Standard Deviation"),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))
```

```{r}
table9 <- qb_eda %>%
  summarize(mean=mean(total_sacks),
            min=min(total_sacks),
            median=median(total_sacks),
            max=max(total_sacks),
            sd=sd(total_sacks))

table9 %>%
  kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Summary Table for Total Sacks",
    col.names = c("Mean", "Minimum", "Median", "Maximum", "Standard Deviation"),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))
```

These summary tables show that many of these variables have very large standard deviations. This must be something to look out for in our regression.

Below are several different side-by-side boxplots that attempt to show some relationships of what we believe to be the two most important stats, passing yards and touchdowns. Both are shown by decade played and by HOF Status. Additionally, they are shown together in a scatterplot. 

```{r}
ggplot(qb_hof, mapping = aes(x=Decade, y=pass_yards))+
  geom_boxplot()+
  labs(x='Decade Majority of Career Spent', y = "Career Passing Yards", title = 'Passing Yards by Decade')

ggplot(qb_hof, mapping = aes(x=Decade, y=TD))+
  geom_boxplot()+
  labs(x='Decade Majority of Career Spent', y = "Career Passing Touchdowns", title = 'Touchdowns by Decade')

ggplot(qb_hof, mapping = aes(x=HOF, y=pass_yards))+
  geom_boxplot()+
  labs(x='HOF Status', y = "Career Passing Yards", title = 'Passing Yards by HOF Status')

ggplot(qb_hof, mapping = aes(x=HOF, y=TD))+
  geom_boxplot()+
  labs(x='HOF Status', y = "Career Passing Touchdowns", title = 'Touchdowns by HOF Status')

ggplot(qb_hof, mapping = aes(x=pass_yards, y= TD, color = HOF))+
  geom_point()+
  geom_smooth(method=lm, se=F)+
  labs(x='Career Passing Yards', y = "Career Passing Touchdowns", legend =  'HOF Status', title = 'Touchdowns by Passing Yards by HOF Status')
```

For both variables, there seems to be almost a parabolic trend. They rise as time increases, but then decrease from the 1990s to the 2000s. We expected the rise, but did not expect the decrease at all. As the NFL has evolved, it has become much more of a passing league. These data do not support that claim, though, which is very interesting. What is not surprising at all, however, is the fact that passing yards and touchdowns for players in the HOF appear to be significantly larger than players who are not. This makes complete sense, as players in the HOF likely had more yards and touchdowns in their careers. The scatterplot reveals that players in the HOF have more touchdowns per passing yard than people not in the HOF. This is seen in the fact that players in the HOF have a steeper slope than players in not in the HOF. A steeper slope, by definition, means that Y increases more as X increases by one. Because yards is on the X axis and TDs are on the Y axis, they therefore have more touchdowns per passing yard. This relationship also makes sense. Passing yards are very important, but one cannot win a game with yards alone. The team that wins the game is the team with more points, not the team with more yards. Touchdowns score points, so it is beneficial for a QB to have more touchdowns per yard.


Below are two scatterplots that show the relationships between interceptions and touchdowns, and between attempts and completion percentage. 

```{r}
ggplot(data=qb_eda, aes(x=total_int, y=total_td)) +
  geom_point(color = "blue") +
  geom_smooth(method=lm, se=FALSE, color = "red") +
  labs(x = "Total Interceptions", 
       y = "Total Touchdowns", 
       title = "Total Touchdowns vs Total Interceptions") +
  theme(
    plot.title = element_text(size = 16L,
                              face = "bold",
                              hjust = 0.5),
    plot.subtitle = element_text(size = 12L,
                                 hjust = 0.5),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12))

ggplot(data=qb_eda, aes(x=attempts, y=completion_pct)) +
  geom_point(color = "blue") +
  geom_smooth(method=lm, se=FALSE, color = "red") +
  labs(x = "Pass Attempts", 
       y = "Pass Completion Percentage", 
       title = "Pass Completion Percentage vs Pass Attempts") +
  theme(
    plot.title = element_text(size = 16L,
                              face = "bold",
                              hjust = 0.5),
    plot.subtitle = element_text(size = 12L,
                                 hjust = 0.5),
    axis.title.x = element_text(size = 12),
    axis.title.y = element_text(size = 12))
```
Both scatterplots show a positive, linear trend. The first shows that as interceptions increase, so do touchdowns. This relationship makes perfect sense. Both interceptions and touchdowns are a result of throwing the ball. It stands to reason that as one throws the ball more, both interceptions and touchdowns would increase. The second scatterplot shows that as pass attempts increase, so does completion percentage. Completion percentage is defined as $\frac{Cmp}{Att} \times 100$. Attempts are in the denominator of that equation, so as they increase, one might expect completion percentage to decrease. The real reason for this increase is likely due to the fact that only the really skilled QBs have that many attempts in the first place, and the more skilled they are the more attempts they tend to have. So, the QBs with the most skill have the most attempts, and because they are so skilled, they also have a very good completion percentage.


## 1.4 Fitting and Analysis

The GLM our group chose is logistic regression. Logistic regression is unlike other types of regression because it requires a categorical variable as its response. Logistic regression estimates the probability of an event occurring. We observed linearity in our EDA, all values are independent from another, and the sample size is large enough. All conditions for logistic regression are met except for multicollinearity. Many of the terms are multicollinear, we will have to proceed with caution.

To select which variables are best for prediction, we will use backwards selection. Backwards selection starts with the full model with all predictors, and systematically removes the predictor that leads to the largest reduction in AIC until removing a term no longer significantly reduces AIC.
```{r include = F}
int_only_model <- glm(HOF_ind ~ 1, family = binomial, data = qb_hof)

full_model <- glm(HOF_ind ~ pass_yards + TD + INT + first_downs + Sck + seasons_played + Decade, family = binomial, data = qb_hof)

stats::step(object = full_model, 
            scope = list(lower = int_only_model, upper = full_model),
            data = qb_hof,
            direction = "backward")
```

The backwards selection chose a model with only three terms, `TD`, `first_downs`, and `seasons_played`. This was very surprising to our group. We expected terms like yards or interceptions to be included, and did not expect first downs or seasons played to be included. Our previous knowledge as football fans led us to believe that other terms would be important. We were also surprised not to see the decade term included, as there seemed to be a relationship with decade seen in our EDA. We decided to proceed with two models, one with decade and one without it. 

Below are the summaries of the two models. 

```{r include = T}
logmodel <- glm(HOF_ind ~ TD + first_downs + seasons_played + Decade, family = binomial, data = qb_hof)

summary(logmodel)

logmodelNoD <- glm(HOF_ind ~ TD + first_downs + seasons_played, family = binomial, data = qb_hof)

summary(logmodelNoD)
```
The decade term is very insignificant, with p-values around or above 0.3. Additionally, the first downs term is very insignificant in the model with decade. It is still insignificant in the model without decade at a 0.05 level, but it is still much lower. These models very surprising to us because many of the coefficient terms are negative. First downs and seasons played are negative, meaning an increase in either decreases a player's probability to make the HOF. Additionally, all decade terms are negative, meaning that players in the 1970s had the highest probability to make the HOF. Perhaps the HOF has gotten more selective over time. The most logical explanation for this is that the standards were simply lower back then. As the league has evolved, passing has become much more frequent. Because of this, better stats in the more recent decades are less impressive, relative to their peers. 

To confirm the backwards selection, a drop-in-deviance test was conducted. The hypotheses are as follows:
$$ 
\begin{align}
H_0 &: \text{The reduced model is preferred.} \\
H_A &: \text{The full model is preferred.}
\end{align}
$$
```{r}
anova(logmodel, full_model)
# confirms backwards selection

anova(logmodelNoD, full_model)
```
Because both p-values are insignificant at a 0.05 level, we fail to reject our null hypothesis and conclude that the reduced model is preferred over the full model. This confirms our backwards selection. 

We want to test if decade really is an important predictor. We will perform another drop-in-deviance test to see which model is preferred. This test uses the same hypotheses as before.

```{r}
anova(logmodelNoD, logmodel)
```
This test returns a p-value of 0.656, which is very insignificant. We fail to reject our null hypothesis and conclude that the reduced model without decade is preferred. Based on the backwards selection and drop-in-deviance test, we will remove decade from our model, despite the fact that some of our EDA indicated it might be important. 

To see how well our model can predict, we want to generalize it to new data. But before that, just as a baseline, we want to test it on the training data first. 
```{r decade, message = F, include = F}
pred_prob <- predict(logmodel, newdata = qb_hof, type = "response")

qb_hof <-
  qb_hof %>%
  mutate(predProbaility = pred_prob)

#Generate predicted values of y (call them pred_surv)
threshold  <- 0.6
pred_HOF <- ifelse(pred_prob > threshold, "Yes", "No")

#Add predictions to dataset
qb_hof <-
  qb_hof %>%
  mutate(Prediction = pred_HOF)

confmat <- qb_hof %>%
  mutate(PredictedProb = pred_prob,
         PredictedHOF = pred_HOF) %>%
  group_by(HOF, PredictedHOF) %>%
  summarize(n = n()) %>%
  spread(HOF, n)

test_roc = roc(response = qb_hof$HOF,
               predictor = pred_prob,
               plot = TRUE, print.auc = TRUE,
               legacy.axes=TRUE)

confmat %>% kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Confusion Matrix",
    col.names = c('Predicted', 'Actual No', 'Actual Yes'),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))

#Accuracy = 0.875
#Sensitivity = 0.6923
#Specificity = 0.9630
```

```{r warning = F, message = F}
pred_probNoD <- predict(logmodelNoD, newdata = qb_hof, type = "response")

qb_hof <-
  qb_hof %>%
  mutate(predProbaility = pred_probNoD)
  
#Generate predicted values of y (call them pred_surv)
threshold  <- 0.6
pred_HOFNoD <- ifelse(pred_probNoD > threshold, "Yes", "No")

#Add predictions to dataset
qb_hof <- 
  qb_hof %>%
  mutate(PredictionNoD = pred_HOFNoD)

confmatNoD <- qb_hof %>%
  mutate(PredictedProb = pred_probNoD,
         PredictedHOF = pred_HOFNoD) %>%
  group_by(HOF, PredictedHOF) %>%
  summarize(n = n()) %>%
  spread(HOF, n) 

test_roc = roc(response = qb_hof$HOF,
               predictor = pred_probNoD, 
               plot = TRUE, print.auc = TRUE, 
               legacy.axes=TRUE)

confmatNoD %>% kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Confusion Matrix no Decade",
    col.names = c('Predicted', 'Actual No', 'Actual Yes'),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))

#Accuracy = 0.9000
#Sensitivity : 0.7692
#Specificity : 0.9630
```
Based on a threshold of 0.6, the resulting AUC is 0.923 and the confusion matrix has an accuracy of 0.9, a sensitivity of 0.7692, and a specificity of 0.9630. The sensitivity or true positive rate is low, but that is likely due to the fact that we simply do not have very many positives in total. Now we are free to generalize to new data. While not included, it is interesting to note that the model with decade has a lower accuracy and sensitivity. 

## 1.5 Generalizing to New Data 

To test the effectiveness of the model, it needs to be tested on data it has not seen before. We will now test the model on the unfiltered dataset, with every QB who played between 1970 and 2016. 
```{r message = F, warning = F}
pred_prob2 <- predict(logmodelNoD, newdata = allqb_hof, type = "response")

allqb_hof <-
  allqb_hof %>%
  mutate(predProbaility2 = pred_prob2)
  
#Generate predicted values of y (call them pred_surv)
threshold  <- 0.6
pred_HOF2 <- ifelse(pred_prob2 > threshold, "Yes", "No")

#Add predictions to dataset
allqb_hof <- 
  allqb_hof %>%
  mutate(Prediction2 = pred_HOF2)

confmat2 <- allqb_hof %>%
  mutate(PredictedProb2 = pred_prob2,
         PredictedHOF2 = pred_HOF2) %>%
  group_by(HOF, PredictedHOF2) %>%
  summarize(n = n()) %>%
  spread(HOF, n) 

test_roc = roc(response = allqb_hof$HOF,
               predictor = pred_prob2, 
               plot = TRUE, print.auc = TRUE, 
               legacy.axes=TRUE)

confmat2 %>% kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Confusion Matrix",
    col.names = c('Predicted', 'Actual No', 'Actual Yes'),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))

#accuracy : 0.9877
#Sensitivity : 0.7692    
#Specificity : 0.9949  

FN <- allqb_hof %>% filter(Prediction2 == 'No', HOF == 'Yes')
FP <- allqb_hof %>% filter(Prediction2 == 'Yes', HOF == 'No')
```
Unfortunately, the sensitivity is still relatively low at 0.7692, which is still due to there being very few players in the HOF in the datatset. A possible reason for the low sensitivity is the fact that our model does not include awards or championships. For example, players like Steve Young or Troy Aikman do not have very impressive stats, but both have 3 or more super bowl victories. These players are false negatives. Accuracy and specificity, 0.9877 and 0.9949 respecively, both increased. Our model is very good at predicting true negatives, likely because the dataset has many negatives in it. The AUC is 0.947, which when combined with the matrix, make our model very generalizable to new data. 

### 1.5.1 Machine Learning

We want to perform machine learning to further test the model. We decided to use the random random forest algorithm. Random forest uses decision trees to make predictions.
```{r warning = F, message = F}
set.seed(123)
rfModel <- randomForest(HOF_ind ~ TD + first_downs + seasons_played, data = qb_hof, ntree = 500, mtry = 2)

rfProb <- predict(rfModel, newdata = allqb_hof, type = "response")
rfPred <- ifelse(rfProb > .6 , "Yes", "No")

test_roc = roc(response = allqb_hof$HOF,
               predictor = rfProb, 
               plot = TRUE, print.auc = TRUE, 
               legacy.axes=TRUE)

confmatRF <- table(rfPred, allqb_hof$HOF)

confmatRF %>% kable(
    digits = 2,
    format.args = list(big.mark = ","),
    caption = "Confusion Matrix Random Forest",
    col.names = c('Predicted', 'Actual No', 'Actual Yes'),
    align = c("l", rep("c", 2)),
    booktabs = TRUE
  ) %>%
  kableExtra::kable_classic(lightable_options = "striped",
                            latex_options = c("scale_down", "HOLD_position"))

#accuracy : 0.9901
#Sensitivity : 0.9231   
#Specificity : 0.9923  
```
At a threshold of 0.6, our random forest had an AUC of 0.931, an accuracy of 0.9901, a sensitivity of 0.9231, and a specificity of 0.9923. The random forest model is very good at making predictions, based on our observed confusion matrix. This is further proof that our model can be generalized very well to new data.

## 1.6 GLM Conclusion

We are satisfied with the overall performance of our model, seen by an accuracy of 99% using the random forest algorithm. Additionally, the area under the curve (AUC) exceeds 0.9, indicating excellent and reliable model performance. In the future, we could incorporate predictors that account for winning percentage in both the regular season and postseason. Contextually, success at the highest level, especially for quarterbacks, is a key factor in determining if a player makes the hall of fame.

# Chapter 2: MLM

```{r}
qb_hof <- qb_hof %>%
  mutate(Decade = as.factor(Decade))  # Decade as group for random effects
```

```{r}
# --- 2. Null (Unconditional Means) Model ---
mlm_model_null <- glmer(HOF_ind ~ 1 + (1 | Decade),
                    data = qb_hof,
                    family = binomial(link = "logit"))

# Calculate ICC
var_decade <- as.numeric(VarCorr(mlm_model_null)$Decade[1])
icc <- var_decade / (var_decade + (pi^2 / 3))
cat("ICC:", round(icc, 3), "\n")
```

```{r}
# --- 3. Random Intercepts Model ---
mlm_model1 <- glmer(HOF_ind ~ TD + first_downs + seasons_played + (1 | Decade),
                data = qb_hof,
                family = binomial(link = "logit"),
                control = glmerControl(optimizer = "bobyqa"))

summary(mlm_model1)
```
Our model shows that touchdowns have a statistically significant positive association with Hall of Fame induction. On the other hand, the number of seasons played shows a significant negative association. As we previously mentioned in the report, this could be a reflection of a player's diminishing returns as they get older.

Looking at the correlation of fixed effects, there is a moderate negative correlation between first_downs and seasons_played. This suggests that as one increases, the other tends to decrease. You can see that both are negatively correlated with the intercept and TD. These correlations supoort that some multicollinearity could be present.


```{r}
# --- 4. Random Slopes Model ---
mlm_model2 <- glmer(HOF_ind ~ TD + first_downs + seasons_played + (TD | Decade),
                data = qb_hof,
                family = binomial(link = "logit"),
                control = glmerControl(optimizer = "bobyqa"))

summary(mlm_model2)
```
The coefficients in Model 2 are the same relative size and have the same relative significance as those in Model 1. This suggests the model may be overfitted or that the random slope for TD is unnecessary. Next, we will use an ANOVA table to compare the two models.

```{r}
# Compare random intercept vs random slope
anova(mlm_model1, mlm_model2)
```
Our results show that both models have equal log-likelihoods (-12.383) and nearly identical deviance values. We can confirm that the added random slope does not significantly improve the model, as the likelihood ratio test yields a Chi-square value of 0 and a p-value of 1. Additionally, Model 1 has lower AIC and BIC values. These findings suggest that Model 1, the simpler model, is the better choice.


```{r}
# --- 5. Predictive Evaluation on Training Data ---
threshold <- 0.6

# Predict probabilities
qb_hof$pred_prob <- predict(mlm_model1, type = "response")
qb_hof$Prediction <- ifelse(qb_hof$pred_prob > threshold, "Yes", "No")

# Confusion Matrix
mlm_confmat <- qb_hof %>%
  mutate(Predicted = Prediction) %>%
  group_by(HOF, Predicted) %>%
  summarise(n = n(), .groups = "drop") %>%
  pivot_wider(names_from = HOF, values_from = n)

confmat %>%
  kable(caption = "Confusion Matrix (Training)",
        col.names = c("Predicted", "Actual No", "Actual Yes"),
        align = c("l", "c", "c"),
        booktabs = TRUE) %>%
  kable_classic("striped", latex_options = "scale_down")

# ROC and AUC
roc_curve <- roc(response = qb_hof$HOF,
                 predictor = qb_hof$pred_prob,
                 plot = TRUE,
                 print.auc = TRUE,
                 legacy.axes = TRUE)
```



```{r}
# --- 6. Predictive Generalization to All QBs ---
allqb_hof$pred_prob <- predict(mlm_model1, newdata = allqb_hof, type = "response", allow.new.levels = TRUE)
allqb_hof$Prediction <- ifelse(allqb_hof$pred_prob > threshold, "Yes", "No")

# Confusion Matrix on Generalization Data
confmat_all <- allqb_hof %>%
  mutate(Predicted = Prediction) %>%
  group_by(HOF, Predicted) %>%
  summarise(n = n(), .groups = "drop") %>%
  pivot_wider(names_from = HOF, values_from = n)

confmat_all %>%
  kable(caption = "Confusion Matrix (Generalization)",
        col.names = c("Predicted", "Actual No", "Actual Yes"),
        align = c("l", "c", "c"),
        booktabs = TRUE) %>%
  kable_classic("striped", latex_options = "scale_down")

# ROC and AUC for Generalization
roc_curve_all <- roc(response = allqb_hof$HOF,
                     predictor = allqb_hof$pred_prob,
                     plot = TRUE,
                     print.auc = TRUE,
                     legacy.axes = TRUE)
```
The model demonstrated strong performance on both the training and generalization datasets. On the training data, it achieved an accuracy of 87.5% and an AUC of 92.3%. The model performed even better on the generalization data, with an accuracy of 98.8% and an AUC of 94.7%. These metrics reflect a highly effective predictive model and one that generalizes exceptionally well.
